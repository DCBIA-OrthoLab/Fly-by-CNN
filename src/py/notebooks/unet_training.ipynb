{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646ba785-1e6d-498d-9973-432228557d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import imageio\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence as pack_sequence, pad_packed_sequence as unpack_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.models as models\n",
    "# io utils\n",
    "from pytorch3d.io import load_obj\n",
    "\n",
    "# datastructures\n",
    "from pytorch3d.structures import Meshes\n",
    "\n",
    "# 3D transformations functions\n",
    "from pytorch3d.transforms import Rotate, Translate\n",
    "\n",
    "# rendering components\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras, look_at_view_transform, look_at_rotation, \n",
    "    RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
    "    SoftSilhouetteShader, HardPhongShader, SoftPhongShader, AmbientLights, PointLights, TexturesUV, TexturesVertex,\n",
    ")\n",
    "\n",
    "import vtk\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "import fly_by_features as fbf\n",
    "from vtk.util.numpy_support import vtk_to_numpy\n",
    "from vtk.util.numpy_support import numpy_to_vtk\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "import monai\n",
    "from monai.data import ITKReader, PILReader\n",
    "from monai.transforms import (\n",
    "    ToTensor, LoadImage, Lambda, AddChannel, RepeatChannel, ScaleIntensityRange, RandSpatialCrop,\n",
    "    Resized, Compose\n",
    ")\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Imports for monai model\n",
    "import logging\n",
    "import tempfile\n",
    "from glob import glob\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from monai.data import ArrayDataset, create_test_image_2d, decollate_batch\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AddChannel,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    RandRotate90,\n",
    "    RandSpatialCrop,\n",
    "    ScaleIntensity,\n",
    "    EnsureType,\n",
    ")\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "print(\"imports done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13926b25-031a-45e2-a413-2d456f66b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the cuda device \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ab33d4-ae8c-4fe5-beb6-239e8b4d3890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a perspective camera.\n",
    "cameras = FoVPerspectiveCameras(device=device)\n",
    "\n",
    "\n",
    "# We will also create a Phong renderer. This is simpler and only needs to render one face per pixel.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=512, \n",
    "    blur_radius=0, \n",
    "    faces_per_pixel=1, \n",
    ")\n",
    "# We can add a point light in front of the object. \n",
    "\n",
    "# lights = AmbientLights(device=device)\n",
    "lights = PointLights(device=device)\n",
    "rasterizer = MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    )\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=rasterizer,\n",
    "    shader=HardPhongShader(device=device, cameras=cameras)\n",
    ")\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FlyByDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def set_env_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        surf = fbf.ReadSurf(df.iloc[idx][\"surf\"])\n",
    "        surf = fbf.GetUnitSurf(surf)\n",
    "        surf, _a, _v = fbf.RandomRotation(surf)\n",
    "\n",
    "        surf = fbf.ComputeNormals(surf)\n",
    "\n",
    "        color_normals = ToTensor(dtype=torch.float32, device=device)(vtk_to_numpy(fbf.GetColorArray(surf, \"Normals\"))/255.0)\n",
    "        verts = ToTensor(dtype=torch.float32, device=device)(vtk_to_numpy(surf.GetPoints().GetData()))\n",
    "        faces = ToTensor(dtype=torch.int64, device=device)(vtk_to_numpy(surf.GetPolys().GetData()).reshape(-1, 4)[:,1:])\n",
    "        region_id = ToTensor(dtype=torch.int64, device=device)(vtk_to_numpy(surf.GetPointData().GetScalars(\"UniversalID\")))\n",
    "        region_id = torch.clamp(region_id, min=0)\n",
    "        faces_pid0 = faces[:,0:1]\n",
    "        region_id_faces = torch.take(region_id, faces_pid0)\n",
    "        #print(\"shape region_id_faces: \", region_id_faces.shape)\n",
    "        \n",
    "        return verts, faces, region_id, region_id_faces, faces_pid0, color_normals,df.iloc[idx][\"surf\"]\n",
    "        \n",
    "def pad_verts_faces(batch):\n",
    "    names = [n for v,f,rid,ridf,fpid0,cn,n in batch]\n",
    "    verts = [v for v, f, rid, ridf, fpid0, cn,n in batch]\n",
    "    faces = [f for v, f, rid, ridf, fpid0, cn,n in batch]\n",
    "    region_ids = [rid for v, f, rid, ridf, fpid0, cn,n in batch]\n",
    "    region_ids_faces = [ridf for v, f, rid, ridf, fpid0, cn,n in batch]\n",
    "    faces_pid0s = [fpid0 for v, f, rid, ridf, fpid0, cn,n in batch]\n",
    "    color_normals = [cn for v, f, rid, ridf, fpid0, cn,n in batch]\n",
    "    \n",
    "    pad_seq_verts = pad_sequence(verts, batch_first=True, padding_value=0.0)\n",
    "    pad_seq_faces = pad_sequence(faces, batch_first=True, padding_value=-1)\n",
    "    pad_seq_rid = pad_sequence(region_ids, batch_first=True, padding_value=0)\n",
    "    pad_seq_faces_pid0s = pad_sequence(faces_pid0s, batch_first=True, padding_value=-1)\n",
    "    pad_seq_cn = pad_sequence(color_normals, batch_first=True, padding_value=0.)\n",
    "    l = [f.shape[0] for f in faces]\n",
    "    \n",
    "    return pad_seq_verts, pad_seq_faces, pad_seq_rid, torch.cat(region_ids_faces), pad_seq_faces_pid0s, pad_seq_cn, l, names\n",
    "        \n",
    "df = pd.read_csv(\"/NIRAL/work/leclercq/data/training_UID.csv\")\n",
    "\n",
    "# Split data between training and validation \n",
    "df_train, df_val = train_test_split(df, test_size=0.1)  \n",
    "\n",
    "# Datasets \n",
    "train_data = FlyByDataset(df_train)\n",
    "val_data = FlyByDataset(df_val)\n",
    "\n",
    "# Dataloaders\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=pad_verts_faces)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True, collate_fn=pad_verts_faces)\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "post_trans = AsDiscrete(argmax=True, to_onehot=True, num_classes=18)\n",
    "post_label = AsDiscrete(to_onehot=True, num_classes=18)\n",
    "\n",
    "# create UNet, DiceLoss and Adam optimizer\n",
    "model = monai.networks.nets.UNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=4,   # images: torch.cuda.FloatTensor[4,224,224,4]\n",
    "    out_channels=33, # background + gum + 16 teeth\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    ").to(device)\n",
    "loss_function = monai.losses.DiceCELoss(to_onehot_y=True,softmax=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 1e-5)\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = list()\n",
    "metric_values = list()\n",
    "writer = SummaryWriter()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b6aa61-3033-4eab-93db-a3d8f6484c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "epoch 1/1000\n",
      "torch.Size([1, 33, 512, 512])\n",
      "torch.Size([1, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:312: operator(): block: [411,0,0], thread: [0,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:312: operator(): block: [411,0,0], thread: [1,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:312: operator(): block: [411,0,0], thread: [2,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:312: operator(): block: [411,0,0], thread: [3,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:312: operator(): block: [711,0,0], thread: [32,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_NOT_INITIALIZED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6953/3094474046.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tools/anaconda3/envs/monai/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tools/anaconda3/envs/monai/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED"
     ]
    }
   ],
   "source": [
    "nb_epoch = 1000\n",
    "dist_cam = 1.35\n",
    "\n",
    "\n",
    "camera_position = ToTensor(dtype=torch.float32, device=device)([[0, 0, dist_cam]])\n",
    "R = look_at_rotation(camera_position, device=device)  # (1, 3, 3)\n",
    "T = -torch.bmm(R.transpose(1, 2), camera_position[:,:,None])[:, :, 0]   # (1, 3)\n",
    "\n",
    "# Start training\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch  = -1\n",
    "epoch_loss_values = list()\n",
    "metric_values = list()\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range (nb_epoch):\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"epoch {epoch + 1}/{nb_epoch}\")\n",
    "    model.train() # Switch to training mode\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch, (V, F, Y, YF, F0, CN, FL,N) in enumerate(train_dataloader):\n",
    "        step += 1\n",
    "        textures = TexturesVertex(verts_features=CN)\n",
    "        meshes = Meshes(verts=V, faces=F, textures=textures)\n",
    "        images = phong_renderer(meshes_world=meshes.clone(), R=R, T=T)\n",
    "        pix_to_face, zbuf, bary_coords, dists = phong_renderer.rasterizer(meshes.clone())\n",
    "        y_p = torch.take(YF, pix_to_face)*(pix_to_face >= 0)\n",
    "        images = images.permute(0,3,1,2)\n",
    "        y_p = y_p.permute(0,3,1,2)\n",
    "        #print(\"shape images : \",images.shape)\n",
    "        #print(\"shape y_p : \",y_p.shape)\n",
    "        inputs, labels = images.to(device), y_p.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        print(outputs.shape)\n",
    "        print(labels.shape)\n",
    "        loss = loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_len = int(np.ceil(len(train_data) / train_dataloader.batch_size))\n",
    "        print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "        writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "        \n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    writer.add_scalar(\"train_mean_dice\", epoch_loss, epoch + 1)\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    if (epoch + 1) % val_interval == 0: # every two epochs : validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_images = None\n",
    "            val_yp = None\n",
    "            val_outputs = None\n",
    "            for batch, (V, F, Y, YF, F0, CN, FL,N) in enumerate(val_dataloader):               \n",
    "\n",
    "                textures = TexturesVertex(verts_features=CN)\n",
    "                meshes = Meshes(verts=V, faces=F, textures=textures)\n",
    "                val_images = phong_renderer(meshes_world=meshes.clone(), R=R, T=T)    \n",
    "                pix_to_face, zbuf, bary_coords, dists = phong_renderer.rasterizer(meshes.clone()) \n",
    "                val_y_p = torch.take(YF, pix_to_face)*(pix_to_face >= 0)\n",
    "                val_images, val_y_p = val_images.permute(0,3,1,2), val_y_p.permute(0,3,1,2)            \n",
    "                val_images, val_labels = val_images.to(device), val_y_p.to(device)\n",
    "                \n",
    "                roi_size = (96, 96)\n",
    "                sw_batch_size = 4\n",
    "                val_outputs = sliding_window_inference(val_images, roi_size, sw_batch_size, model)\n",
    "                \n",
    "                val_labels_list = decollate_batch(val_labels)                \n",
    "                val_labels_convert = [\n",
    "                    post_label(val_label_tensor) for val_label_tensor in val_labels_list\n",
    "                ]\n",
    "                \n",
    "                val_outputs_list = decollate_batch(val_outputs)\n",
    "                val_output_convert = [\n",
    "                    post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list\n",
    "                ]\n",
    "                \n",
    "                dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
    "    \n",
    "                \n",
    "            # aggregate the final mean dice result\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            # reset the status for next validation round\n",
    "            dice_metric.reset()\n",
    "            metric_values.append(metric)\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), \"best_metric_model_segmentation2d_array.pth\")\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                \"current epoch: {} current mean dice: {:.4f} best mean dice: {:.4f} at epoch {}\".format(\n",
    "                    epoch + 1, metric, best_metric, best_metric_epoch\n",
    "                )\n",
    "            )\n",
    "            writer.add_scalar(\"val_mean_dice\", metric, epoch + 1)\n",
    "            \n",
    "            \n",
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "writer.close()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5f27db-8e66-4122-a4fc-6114a28c8d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(\"shape V : \",V.shape)\n",
    "print(\"shape F : \",F.shape)\n",
    "print(\"shape Y : \",Y.shape)\n",
    "print(\"shape YF : \",YF.shape)\n",
    "\n",
    "print(\"shape F0 : \",F0.shape)\n",
    "print(\"shape F0[0] : \",F0[0].shape)\n",
    "\"\"\"\n",
    "print(images.type())\n",
    "print(\"images shape: \",images.shape)\n",
    "print(\"y_p shape: \", y_p.shape)\n",
    "print(len(train_dataloader))\n",
    "print(len(train_dataloader))\n",
    "print(images.type())\n",
    "print(y_p.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b196fad8-5332-487a-b1cb-89373094bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "images = images.permute(0,2,3,1)\n",
    "y_p = y_p.permute(0,2,3,1)\n",
    "\n",
    "fig = go.Figure(make_subplots(rows=1, cols=2, column_widths=[0.5, 0.5], specs=[[{}, {}]]))\n",
    "fig.add_trace(go.Image(z=(images[2][...,0:3]*255).cpu().numpy()), row=1, col=1)\n",
    "\n",
    "labelmap = np.flip((y_p[2][...,0]).cpu().numpy(), axis=0)\n",
    "fig.add_trace(go.Heatmap(z=labelmap), row=1, col=2)\n",
    "fig.update_layout(\n",
    "    width = 1400, height = 700,\n",
    "    autosize = False )\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6a98f-ac50-4efe-bf7d-d8d2304adae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = 1.016516198\n",
    "print(f\"best metric: {a:.4f}\")\n",
    "print(metric_values)\n",
    "a =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceae8b42-1644-4057-8e56-422e024e1431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c6993-0bb1-4d91-b7ab-7853b4fa1359",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.utils.collect_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b43e6d-9706-418f-9e13-3e263bad7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
