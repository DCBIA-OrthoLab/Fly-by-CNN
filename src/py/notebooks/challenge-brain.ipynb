{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e89b07-efa1-42a8-a168-1d3c6e3c989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy   as np\n",
    "import nibabel as nib\n",
    "from fsl.data import gifti\n",
    "from icecream import ic\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "import utils\n",
    "import vtk\n",
    "from vtk.util.numpy_support import vtk_to_numpy, numpy_to_vtk, numpy_to_vtkIdTypeArray\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# rendering components\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras, look_at_view_transform, look_at_rotation, \n",
    "    RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
    "    SoftSilhouetteShader, HardPhongShader, SoftPhongShader, AmbientLights, PointLights, TexturesUV, TexturesVertex, TexturesAtlas\n",
    ")\n",
    "# datastructures\n",
    "from pytorch3d.structures import Meshes\n",
    "\n",
    "import monai\n",
    "from monai.inferers import (sliding_window_inference,SimpleInferer)\n",
    "from monai.transforms import ToTensor\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AddChannel,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    RandRotate90,\n",
    "    RandSpatialCrop,\n",
    "    ScaleIntensity,\n",
    "    EnsureType,\n",
    ")\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd5b9ef-7d7e-4d3e-bae5-4beff936997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainDataset(Dataset):\n",
    "    def __init__(self,np_split,triangles):\n",
    "        self.np_split = np_split\n",
    "        self.triangles = triangles  \n",
    "        self.nb_triangles = len(triangles)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.np_split))\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        data_dir = '/CMF/data/geometric-deep-learning-benchmarking/Data/Segmentation/Native_Space'\n",
    "        #data_dir = '/CMF/data/geometric-deep-learning-benchmarking/Data/Segmentation/Template_Space'\n",
    "        item = self.np_split[idx][0]\n",
    "\n",
    "        # for now just try with Left\n",
    "        path_features = f'{data_dir}/segmentation_template_space_features/{item}_L.shape.gii'\n",
    "        path_labels = f'{data_dir}/segmentation_template_space_labels/{item}_L.label.gii'\n",
    "        \n",
    "        path_features = f'{data_dir}/segmentation_native_space_features/{item}_L.shape.gii'\n",
    "        path_labels = f'{data_dir}/segmentation_native_space_labels/{item}_L.label.gii'\n",
    "        \n",
    "        vertex_features = gifti.loadGiftiVertexData(path_features)[1] # vertex features\n",
    "                \n",
    "        vertex_labels = gifti.loadGiftiVertexData(path_labels)[1] # vertex labels\n",
    "        faces_pid0 = self.triangles[:,0:1]\n",
    "\n",
    "        # vertex_features_0 = vertex_features[:,0]\n",
    "        # vertex_features_1 = vertex_features[:,1]\n",
    "        # vertex_features_2 = vertex_features[:,2]\n",
    "        # vertex_features_3 = vertex_features[:,3]  \n",
    "        \n",
    "        # face_features_0 = np.take(vertex_features_0,faces_pid0)\n",
    "        # face_features_1 = np.take(vertex_features_1,faces_pid0)\n",
    "        # face_features_2 = np.take(vertex_features_2,faces_pid0)\n",
    "        # face_features_3 = np.take(vertex_features_3,faces_pid0)\n",
    "        # l_face_features = [face_features_0,face_features_1,face_features_2,face_features_3]\n",
    "        \n",
    "        # face_features = np.stack(l_face_features,axis=-1)\n",
    "        # face_features = np.squeeze(face_features)        \n",
    "        \n",
    "        \n",
    "        face_labels = np.take(vertex_labels,faces_pid0) # face labels (taking first vertex for each face)        \n",
    "        \n",
    "        #offset = np.arange(self.nb_triangles*4).reshape((self.nb_triangles,4))\n",
    "        offset = np.zeros((self.nb_triangles,4), dtype=int) + np.array([0,1,2,3])\n",
    "        faces_pid0_offset = offset + np.multiply(faces_pid0,4)        \n",
    "        \n",
    "        face_features = np.take(vertex_features,faces_pid0_offset)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        ic(vertex_features.shape)\n",
    "        ic(face_features.shape)\n",
    "        ic(vertex_labels.shape)\n",
    "        ic(face_labels.shape)\n",
    "        ic(faces_pid0.shape)\n",
    "        ic(faces_pid0_offset.shape)\n",
    "        ic(faces_pid0)\n",
    "        ic(faces_pid0_offset)\n",
    "        ic(vertex_features)\n",
    "        ic(face_features)\n",
    "        ic(face_features_test)\n",
    "        \"\"\"\n",
    "        ic(min(face_labels))\n",
    "        ic(max(face_labels))\n",
    "        \n",
    "        return vertex_features,face_features, face_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2165a4-5b4e-4eae-a4d8-7b8faedab18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the cuda device \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# Initialize a perspective camera.\n",
    "cameras = FoVPerspectiveCameras(device=device)\n",
    "\n",
    "\n",
    "# We will also create a Phong renderer. This is simpler and only needs to render one face per pixel.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=512, \n",
    "    blur_radius=0, \n",
    "    faces_per_pixel=1, \n",
    ")\n",
    "# We can add a point light in front of the object. \n",
    "\n",
    "# lights = AmbientLights(device=device)\n",
    "lights = PointLights(device=device)\n",
    "rasterizer = MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    )\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=rasterizer,\n",
    "    shader=HardPhongShader(device=device, cameras=cameras)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0edb26b-dc07-4e91-a846-10810913d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Neural Network model\n",
    "\n",
    "\n",
    "\n",
    "num_classes = 40 #37\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "post_trans = AsDiscrete(argmax=True, to_onehot=num_classes, num_classes=num_classes)\n",
    "post_label = AsDiscrete(to_onehot=num_classes, num_classes=num_classes)\n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=num_classes, num_classes=num_classes)\n",
    "\n",
    "\"\"\"\n",
    "# create UNETR, DiceLoss and Adam optimizer\n",
    "model = monai.networks.nets.UNETR(\n",
    "    spatial_dims=2,\n",
    "    in_channels=4,   # images: torch.cuda.FloatTensor[batch_size,224,224,4]\n",
    "    img_size=image_size,\n",
    "    out_channels=num_classes, \n",
    ").to(device)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# create UNet, DiceLoss and Adam optimizer\n",
    "model = monai.networks.nets.UNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=4,   # images: torch.cuda.FloatTensor[batch_size,224,224,4]\n",
    "    out_channels=num_classes, \n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    ").to(device)\n",
    "\n",
    "# model.load_state_dict(torch.load(\"early_stopping/checkpoint_1.pt\"))\n",
    "\n",
    "loss_function = monai.losses.DiceCELoss(to_onehot_y=True,softmax=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 1e-4)\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = list()\n",
    "metric_values = list()\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de3e9c7d-f0bd-4baa-98e9-608ea7841e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ico = '/CMF/data/geometric-deep-learning-benchmarking/Icospheres/ico-6.surf.gii'\n",
    "\n",
    "#mesh = gifti.loadGiftiMesh(path_ico)\n",
    "\n",
    "# load icosahedron\n",
    "ico_surf = nib.load(path_ico)\n",
    "\n",
    "# extract points and faces\n",
    "coords = ico_surf.agg_data('pointset')\n",
    "triangles = ico_surf.agg_data('triangle')\n",
    "nb_faces = len(triangles)\n",
    "connectivity = triangles.reshape(nb_faces*3,1) # 3 points per triangle\n",
    "connectivity = np.int64(connectivity)\t\n",
    "offsets = [3*i for i in range (nb_faces)]\n",
    "offsets.append(nb_faces*3) #  The last value is always the length of the Connectivity array.\n",
    "offsets = np.array(offsets)\n",
    "\n",
    "# rescale icosphere [0,1]\n",
    "coords = np.multiply(coords,0.01)\n",
    "\n",
    "# convert to vtk\n",
    "vtk_coords = vtk.vtkPoints()\n",
    "vtk_coords.SetData(numpy_to_vtk(coords))\n",
    "vtk_triangles = vtk.vtkCellArray()\n",
    "vtk_offsets = numpy_to_vtkIdTypeArray(offsets)\n",
    "vtk_connectivity = numpy_to_vtkIdTypeArray(connectivity)\n",
    "vtk_triangles.SetData(vtk_offsets,vtk_connectivity)\n",
    "\n",
    "\"\"\"\n",
    "# Create icosahedron as a VTK polydata \n",
    "ico_polydata = vtk.vtkPolyData() # initialize polydata\n",
    "ico_polydata.SetPoints(vtk_coords) # add points\n",
    "ico_polydata.SetPolys(vtk_triangles) # add polys\n",
    "\"\"\"\n",
    "\n",
    "# convert ico verts / faces to tensor\n",
    "ico_verts = torch.from_numpy(coords).unsqueeze(0).to(device)\n",
    "ico_faces = torch.from_numpy(triangles).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "# load train / test splits\n",
    "train_split_path = '/CMF/data/geometric-deep-learning-benchmarking/Train_Val_Test_Splits/Segmentation/M-CRIB-S_train_TEA.npy'\n",
    "val_split_path = '/CMF/data/geometric-deep-learning-benchmarking/Train_Val_Test_Splits/Segmentation/M-CRIB-S_val_TEA.npy'\n",
    "train_split = np.load(train_split_path)\n",
    "val_split = np.load(val_split_path)\n",
    "train_dataset = BrainDataset(train_split,triangles)\n",
    "val_dataset = BrainDataset(val_split,triangles)\n",
    "\n",
    "# Setup training parameters\n",
    "dist_cam = 2\n",
    "batch_size =4\n",
    "nb_epochs = 1_000\n",
    "nb_loops = 12\n",
    "\n",
    "# Match icosphere vertices and faces tensor with batch size\n",
    "l_ico_verts = []\n",
    "l_ico_faces = []\n",
    "for i in range(batch_size):\n",
    "    l_ico_verts.append(ico_verts)\n",
    "    l_ico_faces.append(ico_faces)    \n",
    "ico_verts = torch.cat(l_ico_verts,dim=0)\n",
    "ico_faces = torch.cat(l_ico_faces,dim=0)\n",
    "\n",
    "    \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c875887-b86e-4529-8dc9-08f9cc4c23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| min(face_labels): array([0], dtype=int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| max(face_labels): array([36], dtype=int32)\n",
      "ic| min(face_labels): array([0], dtype=int32)\n",
      "ic| max(face_labels): array([36], dtype=int32)\n",
      "ic| min(face_labels): array([0], dtype=int32)\n",
      "ic| max(face_labels): array([36], dtype=int32)\n",
      "ic| min(face_labels): array([0], dtype=int32)\n",
      "ic| max(face_labels): array([36], dtype=int32)\n",
      "ic| vertex_features.shape: torch.Size([4, 40962, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20, train_loss: 4.9961\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nb_epochs):\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"epoch {epoch + 1}/{nb_epochs}\")\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch, (vertex_features, face_features, face_labels) in enumerate(train_dataloader):\n",
    "        \n",
    "        vertex_features = vertex_features.to(device)\n",
    "        vertex_features = vertex_features[:,:,0:3]\n",
    "        face_labels = torch.squeeze(face_labels,0)\n",
    "        face_labels = face_labels.to(device)\n",
    "        face_features = face_features.to(device)\n",
    "\n",
    "        step += 1\n",
    "        for s in range(nb_loops):\n",
    "            textures = TexturesVertex(verts_features=vertex_features)\n",
    "            ic(vertex_features.shape)\n",
    "            meshes = Meshes(\n",
    "                verts=ico_verts,   \n",
    "                faces=ico_faces, \n",
    "                textures=textures\n",
    "            )\n",
    "            array_coord = np.random.normal(0, 1, 3)\n",
    "            array_coord *= dist_cam/(np.linalg.norm(array_coord))\n",
    "            camera_position = ToTensor(dtype=torch.float32, device=device)([array_coord.tolist()])\n",
    "            R = look_at_rotation(camera_position, device=device)  # (1, 3, 3)\n",
    "            T = -torch.bmm(R.transpose(1, 2), camera_position[:,:,None])[:, :, 0]   # (1, 3)\n",
    "\n",
    "            images = phong_renderer(meshes_world=meshes.clone(), R=R, T=T)    \n",
    "            pix_to_face, zbuf, bary_coords, dists = phong_renderer.rasterizer(meshes.clone())  \n",
    "\n",
    "\n",
    "            #inputs = torch.take(face_features,pix_to_face)*(pix_to_face >= 0) \n",
    "            labels = torch.take(face_labels, pix_to_face)*(pix_to_face >= 0) \n",
    "\n",
    "            l_inputs = []\n",
    "            for index in range(4):\n",
    "                l_inputs.append(torch.take(face_features[:,:,index],pix_to_face)*(pix_to_face >= 0)) # take each feature        \n",
    "\n",
    "            inputs = torch.cat(l_inputs,dim=3)\n",
    "            \n",
    "            inputs = inputs.permute(0,3,1,2)\n",
    "            labels = labels.permute(0,3,1,2)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_len = int(np.ceil(len(train_dataloader) / train_dataloader.batch_size))\n",
    "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "            break\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        ic(ico_verts.shape)\n",
    "        ic(ico_faces.shape)\n",
    "        ic(vertex_features.shape)\n",
    "        ic(face_features.shape)\n",
    "        ic(face_labels.shape)\n",
    "        ic(type(ico_verts))\n",
    "        ic(type(ico_faces))\n",
    "        ic(type(vertex_features))   \n",
    "        ic(inputs.shape)\n",
    "        ic(face_labels)\n",
    "        \"\"\"\n",
    "        break\n",
    "    break\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b074ca-9755-49d5-b303-d0f02f7d7c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(make_subplots(rows=1, cols=2, column_widths=[0.5, 0.5], specs=[[{}, {}]]))\n",
    "fig.add_trace(go.Image(z=(images[0][...,0:3]*255).cpu().numpy()), row=1, col=1)\n",
    "\n",
    "labelmap = np.flip((y_p[0][...,0]).cpu().numpy(), axis=0)\n",
    "fig.add_trace(go.Heatmap(z=labelmap), row=1, col=2)\n",
    "fig.update_layout(\n",
    "    width = 1400, height = 700,\n",
    "    autosize = False )\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ce4c1-703a-4654-949a-7692af2c099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.array([\n",
    "    [1,2,3,4,5],\n",
    "    [6,7,8,9,10]\n",
    "             ])\n",
    "indices = np.array([[0],\n",
    "                   [5]])\n",
    "np.take(l,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14915f31-08f2-48d7-a87b-d6eb5d7f9df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_triangles = len(triangles)\n",
    "l = np.array([6,7,8,9,10])\n",
    "l = torch.from_numpy(l)\n",
    "l[0:1]\n",
    "offset = np.arange(nb_triangles*4).reshape((nb_triangles,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db7874-d407-46ea-9de1-192b91db8bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((nb_triangles,4)) + np.array([0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ecf67f-58c4-4afe-b9f6-8e59ccae3dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
