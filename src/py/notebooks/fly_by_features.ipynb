{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646ba785-1e6d-498d-9973-432228557d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import imageio\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence as pack_sequence, pad_packed_sequence as unpack_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.models as models\n",
    "# io utils\n",
    "from pytorch3d.io import load_obj\n",
    "\n",
    "# datastructures\n",
    "from pytorch3d.structures import Meshes\n",
    "\n",
    "# 3D transformations functions\n",
    "from pytorch3d.transforms import Rotate, Translate\n",
    "\n",
    "# rendering components\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras, look_at_view_transform, look_at_rotation, \n",
    "    RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
    "    SoftSilhouetteShader, HardPhongShader, SoftPhongShader, AmbientLights, PointLights, TexturesUV, TexturesVertex,\n",
    ")\n",
    "\n",
    "import vtk\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "import fly_by_features as fbf\n",
    "from vtk.util.numpy_support import vtk_to_numpy\n",
    "from vtk.util.numpy_support import numpy_to_vtk\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "import monai\n",
    "from monai.data import ITKReader, PILReader\n",
    "from monai.transforms import (\n",
    "    ToTensor, LoadImage, Lambda, AddChannel, RepeatChannel, ScaleIntensityRange, RandSpatialCrop,\n",
    "    Resized, Compose\n",
    ")\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13926b25-031a-45e2-a413-2d456f66b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the cuda device \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "393adff0-73b9-4510-bc94-b5356dc95d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    " \n",
    "    def forward(self, input_seq):\n",
    "        assert len(input_seq.size()) > 2\n",
    " \n",
    "        # reshape input data --> (samples * timesteps, input_size)\n",
    "        # squash timesteps\n",
    "\n",
    "        size = input_seq.size()\n",
    "\n",
    "        batch_size = size[0]\n",
    "        time_steps = size[1]\n",
    "\n",
    "        size_reshape = [batch_size*time_steps] + list(size[2:])\n",
    "        reshaped_input = input_seq.contiguous().view(size_reshape)\n",
    " \n",
    "        output = self.module(reshaped_input)\n",
    "        \n",
    "        output_size = output.size()\n",
    "        output_size = [batch_size, time_steps] + list(output_size[1:])\n",
    "        output = output.contiguous().view(output_size)\n",
    "\n",
    "        return output\n",
    "\n",
    "# class MoveNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MoveNet, self).__init__()\n",
    "\n",
    "#         resnet = models.resnet34(pretrained=True)\n",
    "#         resnet.fc = Identity()\n",
    "#         resnet.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "#         self.resnet = resnet\n",
    "        \n",
    "#         self.MovePrediction = nn.Linear(512, 6)\n",
    "        \n",
    "#     def forward(self, x):\n",
    " \n",
    "#         x = self.resnet(x)\n",
    "#         x = self.MovePrediction(x)\n",
    "\n",
    "#         return x\n",
    "    \n",
    "class MoveNet(nn.Module):\n",
    "    def __init__(self, renderer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.renderer = renderer\n",
    "        \n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        resnet.fc = Identity()\n",
    "        resnet.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet = resnet\n",
    "        \n",
    "        self.MovePrediction = nn.Linear(512, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x, meshes, camera_position, focal_point):\n",
    "        \n",
    "        x = self.resnet(x)\n",
    "        x = self.MovePrediction(x)\n",
    "        \n",
    "        camera_position += x[:,0:3]\n",
    "        focal_point += x[:,3:6]\n",
    "        \n",
    "        R = look_at_rotation(camera_position, at=focal_point).to(camera_position.device)  # (1, 3, 3)\n",
    "        T = -torch.bmm(R.transpose(1, 2), camera_position[:,:,None])[:, :, 0]   # (1, 3)\n",
    "        \n",
    "        image = self.renderer(meshes_world=meshes.clone(), R=R, T=T)\n",
    "\n",
    "        pix_to_face, zbuf, bary_coords, dists = self.renderer.rasterizer(meshes)\n",
    "\n",
    "        img_regionid_pid = torch.take(faces_pid0, pix_to_face.reshape(-1))\n",
    "        y_p = torch.take(region_id, img_regionid_pid.to(torch.int64))*(pix_to_face.reshape(-1)>= 0).to(torch.float32)\n",
    "\n",
    "        x = torch.cat([image[..., 0:3], zbuf], dim=-1).permute(0, 3, 1, 2)\n",
    "        \n",
    "        return x.detach(), y_p.detach(), pix_to_face, camera_position, focal_point\n",
    "    \n",
    "class PaintNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PaintNet, self).__init__()\n",
    "\n",
    "        self.unet = monai.networks.nets.UNETR(in_channels=4, out_channels=4, img_size=224, feature_size=32, spatial_dims=2)\n",
    "        \n",
    " \n",
    "    def forward(self, x):\n",
    " \n",
    "        x = self.unet(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def paint_surface(surf, pids, value, neighbors=2):    \n",
    "    \n",
    "    all_pids = pids\n",
    "    for l in range(neighbors):\n",
    "        pids = fbf.GetAllNeighbors(surf, pids)\n",
    "        all_pids = np.concatenate((all_pids, pids))\n",
    "\n",
    "    all_pids = np.unique(all_pids)\n",
    "\n",
    "    for pid in all_pids:\n",
    "        surf.GetPointData().GetScalars().SetTuple1(pid, value)\n",
    "\n",
    "    surf.GetPointData().GetScalars().Modified()\n",
    "\n",
    "def random_choice(x):\n",
    "    idx = torch.randint(x.size(0), (50,))\n",
    "    x = x[idx]\n",
    "    return x\n",
    "\n",
    "def GetColorTable(surf, property_name, range_scalars = None):\n",
    "\n",
    "    if range_scalars == None:\n",
    "        range_scalars = surf.GetPointData().GetScalars(property_name).GetRange()\n",
    "\n",
    "    hueLut = vtk.vtkLookupTable()\n",
    "    hueLut.SetTableRange(0, range_scalars[1])\n",
    "    hueLut.SetHueRange(0.0, 0.9)\n",
    "    hueLut.SetSaturationRange(1.0, 1.0)\n",
    "    hueLut.SetValueRange(1.0, 1.0)\n",
    "    hueLut.Build()\n",
    "    \n",
    "    colors = []\n",
    "    for i in range(int(range_scalars[1] + 1)):\n",
    "        color = [0,0,0]\n",
    "        hueLut.GetColor(i, color)\n",
    "        colors.append(color)\n",
    "    \n",
    "    return np.array(colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5eacaba-c6a1-48c9-8df2-b80fc9335ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 0\n",
    "\n",
    "# surf = fbf.ReadSurf(df.iloc[idx][\"surf\"])\n",
    "# surf = fbf.GetUnitSurf(surf)\n",
    "# surf, _a, _v = fbf.RandomRotation(surf)\n",
    "\n",
    "# surf = fbf.ComputeNormals(surf)\n",
    "\n",
    "# verts = ToTensor(dtype=torch.float32, device=device)(vtk_to_numpy(surf.GetPoints().GetData()))\n",
    "# faces = ToTensor(dtype=torch.int32, device=device)(vtk_to_numpy(surf.GetPolys().GetData()).reshape(-1, 4)[:,1:])\n",
    "# region_id = ToTensor(dtype=torch.int64, device=device)(vtk_to_numpy(surf.GetPointData().GetScalars(\"RegionId\")))\n",
    "# region_id = torch.clamp(region_id, min=0)\n",
    "# faces_pid0 = faces[:,0:1]\n",
    "# color_normals = ToTensor(dtype=torch.float32, device=device)(vtk_to_numpy(fbf.GetColorArray(surf, \"Normals\"))/255.0)\n",
    "# color_points = ToTensor(dtype=torch.float32, device=device)(GetColorTable(surf, \"RegionId\"))\n",
    "# color_points = torch.index_select(color_points, dim=0, index=region_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a19a1cd7-5807-4284-88d3-e624f523e2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# textures = TexturesVertex(verts_features=[color_points])\n",
    "# mesh = Meshes(\n",
    "#     verts=[verts],   \n",
    "#     faces=[faces], \n",
    "#     textures=textures\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35af46a1-9ed1-413d-a280-2b2c38ba39f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a perspective camera.\n",
    "cameras = FoVPerspectiveCameras(device=device)\n",
    "\n",
    "# To blend the 100 faces we set a few parameters which control the opacity and the sharpness of \n",
    "# edges. Refer to blending.py for more details. \n",
    "blend_params = BlendParams(sigma=1e-4, gamma=1e-4)\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# 256x256. To form the blended image we use 100 faces for each pixel. We also set bin_size and max_faces_per_bin to None which ensure that \n",
    "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for \n",
    "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of \n",
    "# the difference between naive and coarse-to-fine rasterization. \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=224, \n",
    "    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma, \n",
    "    faces_per_pixel=100, \n",
    ")\n",
    "\n",
    "# Create a silhouette mesh renderer by composing a rasterizer and a shader. \n",
    "silhouette_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftSilhouetteShader(blend_params=blend_params)\n",
    ")\n",
    "\n",
    "\n",
    "# We will also create a Phong renderer. This is simpler and only needs to render one face per pixel.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=224, \n",
    "    blur_radius=0, \n",
    "    faces_per_pixel=1, \n",
    ")\n",
    "# We can add a point light in front of the object. \n",
    "\n",
    "# lights = AmbientLights(device=device)\n",
    "lights = PointLights(device=device)\n",
    "rasterizer = MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    )\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=rasterizer,\n",
    "    shader=HardPhongShader(device=device, cameras=cameras)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6056a2b1-562a-41ed-a429-ad26f4dbb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select the viewpoint using spherical angles  \n",
    "# distance = 2   # distance from camera to the object\n",
    "# elevation = 90.0   # angle of elevation in degrees\n",
    "# azimuth = 0.0  # No rotation so the camera is positioned on the +Z axis. \n",
    "\n",
    "# # Get the position of the camera based on the spherical angles\n",
    "# R, T = look_at_view_transform(distance, elevation, azimuth, device=device)\n",
    "\n",
    "# # Render the teapot providing the values of R and T. \n",
    "# silhouette = silhouette_renderer(meshes_world=mesh, R=R, T=T)\n",
    "# image_ref = phong_renderer(meshes_world=mesh, R=R, T=T)\n",
    "\n",
    "# silhouette = silhouette.cpu().numpy()\n",
    "# image_ref = image_ref.cpu().numpy()\n",
    "\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(silhouette.squeeze()[..., 3])  # only plot the alpha channel of the RGBA image\n",
    "# plt.grid(False)\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(image_ref.squeeze())\n",
    "# plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "401ff17c-33ff-4c81-8596-dce8fbda9173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pix_to_face, zbuf, bary_coords, dists = rasterizer(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92f1d275-ca22-418c-8641-0c7451750cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(zbuf.squeeze().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d331219-0c4a-4e89-a49d-44ce9af2034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_regionid_pid = torch.take(faces_pid0, pix_to_face.reshape(-1))\n",
    "# img_regionid = torch.take(region_id, img_regionid_pid.to(torch.int64))*(pix_to_face.reshape(-1)>= 0).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7acf76ed-00f4-450c-96e6-40611bc6a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(img_regionid.reshape(224, 224, 1).cpu().numpy(), cmap='gray', vmin=0, vmax=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec79ec62-b83b-4c48-acfb-12d382303362",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "take(): Expected a long tensor for index, but got Int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6577/3953919431.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# img_regionid_pid = torch.gather()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0my_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mirp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mirp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_regionid_pid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0my_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpix_to_face\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6577/3953919431.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# img_regionid_pid = torch.gather()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0my_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mirp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mirp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_regionid_pid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0my_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpix_to_face\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: take(): Expected a long tensor for index, but got Int"
     ]
    }
   ],
   "source": [
    "# Initialize a perspective camera.\n",
    "cameras = FoVPerspectiveCameras(device=device)\n",
    "\n",
    "\n",
    "# We will also create a Phong renderer. This is simpler and only needs to render one face per pixel.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=224, \n",
    "    blur_radius=0, \n",
    "    faces_per_pixel=1, \n",
    ")\n",
    "# We can add a point light in front of the object. \n",
    "\n",
    "# lights = AmbientLights(device=device)\n",
    "lights = PointLights(device=device)\n",
    "rasterizer = MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    )\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=rasterizer,\n",
    "    shader=HardPhongShader(device=device, cameras=cameras)\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FlyByDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def set_env_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        surf = fbf.ReadSurf(df.iloc[idx][\"surf\"])\n",
    "        surf = fbf.GetUnitSurf(surf)\n",
    "        surf, _a, _v = fbf.RandomRotation(surf)\n",
    "\n",
    "        surf = fbf.ComputeNormals(surf)\n",
    "\n",
    "        color_normals = ToTensor(dtype=torch.float32, device=device)(vtk_to_numpy(fbf.GetColorArray(surf, \"Normals\"))/255.0)\n",
    "        verts = ToTensor(dtype=torch.float32, device=device)(vtk_to_numpy(surf.GetPoints().GetData()))\n",
    "        faces = ToTensor(dtype=torch.int32, device=device)(vtk_to_numpy(surf.GetPolys().GetData()).reshape(-1, 4)[:,1:])\n",
    "        region_id = ToTensor(dtype=torch.int64, device=device)(vtk_to_numpy(surf.GetPointData().GetScalars(\"UniversalID\")))\n",
    "        region_id = torch.clamp(region_id, min=0)\n",
    "        faces_pid0 = faces[:,0:1]\n",
    "        \n",
    "        return verts, faces, region_id, faces_pid0, color_normals\n",
    "        \n",
    "def pad_verts_faces(batch):\n",
    "    verts = [v for v, f, rid, fpid0, cn in batch]\n",
    "    faces = [f for v, f, rid, fpid0, cn in batch]\n",
    "    region_ids = [rid for v, f, rid, fpid0, cn in batch]\n",
    "    faces_pid0s = [fpid0 for v, f, rid, fpid0, cn in batch]\n",
    "    color_normals = [cn for v, f, rid, fpid0, cn in batch]\n",
    "    return pad_sequence(verts, batch_first=True, padding_value=0.0), pad_sequence(faces, batch_first=True, padding_value=-1), pad_sequence(region_ids, batch_first=True, padding_value=0), pad_sequence(faces_pid0s, batch_first=True, padding_value=-1), pad_sequence(color_normals, batch_first=True, padding_value=0.)\n",
    "\n",
    "        \n",
    "df = pd.read_csv(\"/ASD/juan_flyby/DCBIA/UniversalId_train.csv\")\n",
    "df_train, df_val = train_test_split(df, test_size=0.1)\n",
    "                 \n",
    "train_data = FlyByDataset(df_train)\n",
    "val_data = FlyByDataset(df_val)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=pad_verts_faces)\n",
    "\n",
    "for batch, (V, F, Y, F0, CN) in enumerate(train_dataloader):\n",
    "    \n",
    "    textures = TexturesVertex(verts_features=CN)\n",
    "    meshes = Meshes(\n",
    "        verts=V,   \n",
    "        faces=F, \n",
    "        textures=textures\n",
    "    )\n",
    "    \n",
    "    # camera_positions = ToTensor(dtype=torch.float32, device=device)([[0, 0, 4]]).repeat(8, 1)\n",
    "    camera_position = ToTensor(dtype=torch.float32, device=device)([[0, 0, 1.25]])\n",
    "    R = look_at_rotation(camera_position, device=device)  # (1, 3, 3)\n",
    "    T = -torch.bmm(R.transpose(1, 2), camera_position[:,:,None])[:, :, 0]   # (1, 3)\n",
    "    \n",
    "    images = phong_renderer(meshes_world=meshes.clone(), R=R, T=T)\n",
    "    \n",
    "    pix_to_face, zbuf, bary_coords, dists = phong_renderer.rasterizer(meshes)\n",
    "\n",
    "    img_regionid_pid = [torch.take(F0[idx], ptf) for idx, ptf in enumerate(pix_to_face)]\n",
    "    # img_regionid_pid = torch.take(F0, pix_to_face).to(torch.int64)\n",
    "    # img_regionid_pid = torch.gather()\n",
    "    \n",
    "    y_p = torch.stack([torch.take(Y[idx], irp) for idx, irp in enumerate(img_regionid_pid)])\n",
    "    y_p = y_p*(pix_to_face >= 0).to(torch.int64)\n",
    "            \n",
    "\n",
    "    # x = torch.cat([image[..., 0:3], zbuf], dim=-1).permute(0, 3, 1, 2)\n",
    "    \n",
    "\n",
    "    print(images.shape, y_p.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e64e6f-e7f7-49e8-a017-6c3828065300",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(make_subplots(rows=1, cols=2, column_widths=[0.5, 0.5], specs=[[{}, {}]]))\n",
    "fig.add_trace(go.Image(z=(images[3][...,0:3]*255).cpu().numpy()), row=1, col=1)\n",
    "\n",
    "labelmap = np.flip((y_p[3][...,0]).cpu().numpy(), axis=0)\n",
    "fig.add_trace(go.Heatmap(z=labelmap), row=1, col=2)\n",
    "fig.update_layout(\n",
    "    width = 1400, height = 700,\n",
    "    autosize = False )\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2f70d-f5cc-4f9b-be5d-34ebda81f411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
